# Decision-Trees-and-Random-Forests
My 5th Project in AI/ML Internship

# ğŸŒ³ Decision Trees & Random Forests - AI & ML Internship Task 5

This project is part of an AI & ML Internship where I learned about tree-based models using the Heart Disease Dataset from Kaggle. The focus was on training and evaluating **Decision Trees** and **Random Forests** for classification.

---

## ğŸ“Œ Task Objectives

- âœ… Train a **Decision Tree Classifier**
- âœ… Visualize the Decision Tree using `sklearn.tree.plot_tree()`
- âœ… Analyze and control **overfitting** by tuning tree depth
- âœ… Train a **Random Forest Classifier** and compare accuracy
- âœ… Interpret **feature importances**
- âœ… Evaluate model performance using **cross-validation**

---

## ğŸ§  What I Learned

- How decision trees split data using entropy and information gain
- Why Random Forests reduce overfitting and improve performance
- The concept of **bagging** and how ensembles work
- How to visualize and interpret tree-based models
- Importance of **feature importance** and model evaluation

---

## ğŸ›  Tools Used

- ğŸ Python
- ğŸ“Š Pandas, Matplotlib, Seaborn
- ğŸ¤– Scikit-learn (`DecisionTreeClassifier`, `RandomForestClassifier`) / Can use Graphviz as well but I did not ğŸ˜Š
- ğŸ“ˆ Jupyter Notebook for experiments and visualization

---


## ğŸ“¸ Visualizations

- ğŸ“¤ Decision Tree plotted directly using `sklearn.tree.plot_tree()`
- ğŸŒ² Feature importance visualized as bar charts
- ğŸ§ª Overfitting analysis using train/test accuracy vs tree depth



