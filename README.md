# Decision-Trees-and-Random-Forests
My 5th Project in AI/ML Internship

# 🌳 Decision Trees & Random Forests - AI & ML Internship Task 5

This project is part of an AI & ML Internship where I learned about tree-based models using the Heart Disease Dataset from Kaggle. The focus was on training and evaluating **Decision Trees** and **Random Forests** for classification.

---

## 📌 Task Objectives

- ✅ Train a **Decision Tree Classifier**
- ✅ Visualize the Decision Tree using `sklearn.tree.plot_tree()`
- ✅ Analyze and control **overfitting** by tuning tree depth
- ✅ Train a **Random Forest Classifier** and compare accuracy
- ✅ Interpret **feature importances**
- ✅ Evaluate model performance using **cross-validation**

---

## 🧠 What I Learned

- How decision trees split data using entropy and information gain
- Why Random Forests reduce overfitting and improve performance
- The concept of **bagging** and how ensembles work
- How to visualize and interpret tree-based models
- Importance of **feature importance** and model evaluation

---

## 🛠 Tools Used

- 🐍 Python
- 📊 Pandas, Matplotlib, Seaborn
- 🤖 Scikit-learn (`DecisionTreeClassifier`, `RandomForestClassifier`) / Can use Graphviz as well but I did not 😊
- 📈 Jupyter Notebook for experiments and visualization

---


## 📸 Visualizations

- 📤 Decision Tree plotted directly using `sklearn.tree.plot_tree()`
- 🌲 Feature importance visualized as bar charts
- 🧪 Overfitting analysis using train/test accuracy vs tree depth



